{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# set environment variable here.\n",
    "os.environ[\"nnUNet_preprocessed\"] = \"/home/bhatti_uhn/nnUNet_preprocessed\"\n",
    "os.environ[\"nnUNet_results\"] = \"/home/bhatti_uhn/nnUNet_results\"\n",
    "os.environ[\"nnUNet_raw\"] = \"/home/bhatti_uhn/nnUNet_raw\"\n",
    "\n",
    "# Ensure that environment variables are set correctly # from run_training.py\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "from nnunetv2.dataset_conversion import generate_dataset_json\n",
    "from nnunetv2.run.run_training import run_training_entry\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "base_dir = '/home/bhatti_uhn/Dataset/UHN-MedImg3D-ML-quiz'\n",
    "nnunet_base = '/home/bhatti_uhn/nnUNet_raw/Dataset876_UHNMedImg3D'\n",
    "\n",
    "images_tr_dir = os.path.join(nnunet_base, 'imagesTr')\n",
    "labels_tr_dir = os.path.join(nnunet_base, 'labelsTr')\n",
    "images_ts_dir = os.path.join(nnunet_base, 'imagesTs')\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(images_tr_dir, exist_ok=True)\n",
    "os.makedirs(labels_tr_dir, exist_ok=True)\n",
    "os.makedirs(images_ts_dir, exist_ok=True)\n",
    "    \n",
    "\n",
    "trainSrc = os.path.join(base_dir, 'train')\n",
    "testSrc = os.path.join(base_dir, 'test')\n",
    "\n",
    "# list subdirectories of trainSrc # remove .DS_Store\n",
    "train_subdirs = [sdir for sdir in os.listdir(trainSrc) if not sdir.startswith('.')]\n",
    "train_subdirs = sorted(train_subdirs)\n",
    "train_subdirs\n",
    "\n",
    "def copy_files_to_raw(images_tr_dir, labels_tr_dir, trainSrc, train_subdirs):\n",
    "\n",
    "    for i, subdir in enumerate(train_subdirs):\n",
    "        print(f\"Processing {subdir}...\")\n",
    "\n",
    "    # create path for reading subdirectories\n",
    "        train_subdir = os.path.join(trainSrc, subdir)\n",
    "\n",
    "    # list files in subdirectory\n",
    "        files = [s for s in sorted(os.listdir(train_subdir)) if not s.startswith('.')]\n",
    "\n",
    "    # separate images and labels files: image: 'quiz_2_002_0000.nii.gz', and label: 'quiz_2_002.nii.gz'\n",
    "        for j, fileName in enumerate(files):\n",
    "            if len(fileName.split('_')) == 4: # assuming quiz_subType_patID is a valid CASE_ID. and 0000 is the modality.\n",
    "                fileNameSplit = fileName.split('_')\n",
    "                logging.info(f\"Processing {fileNameSplit}\")\n",
    "\n",
    "                newFileName = fileNameSplit[0] + '_' + fileNameSplit[1] + '_' + fileNameSplit[2] + '_' + fileNameSplit[3]\n",
    "                tr_copyPath = os.path.join(train_subdir, fileName)\n",
    "\n",
    "                logging.info(f\"Copying {tr_copyPath} to {images_tr_dir}\")\n",
    "                shutil.copy(tr_copyPath, images_tr_dir)\n",
    "                # rename the file\n",
    "                # os.rename(os.path.join(images_tr_dir, fileName), os.path.join(images_tr_dir, newFileName))\n",
    "\n",
    "            elif len(fileName.split('_')) == 3:\n",
    "                fileNameSplit = fileName.split('_')\n",
    "                newFileName = fileNameSplit[0] + '_' + fileNameSplit[1] + '_' + fileNameSplit[2]\n",
    "\n",
    "                shutil.copy(os.path.join(train_subdir, fileName), labels_tr_dir)\n",
    "                # rename the file\n",
    "                # os.rename(os.path.join(labels_tr_dir, fileName), os.path.join(labels_tr_dir, newFileName))\n",
    "\n",
    "            else: \n",
    "                raise ValueError(f\"File {fileName} does not match the expected format.\")\n",
    "\n",
    "\n",
    "def copy_testFiles_to_raw(images_ts_dir, testSrc):\n",
    "\n",
    "    files = [s for s in sorted(os.listdir(testSrc)) if not s.startswith('.')]\n",
    "\n",
    "    for j, fileName in enumerate(files):\n",
    "        shutil.copy(os.path.join(testSrc, fileName), images_ts_dir)\n",
    "\n",
    "# ## uncomment to run ###\n",
    "# copy_files_to_raw(images_tr_dir, labels_tr_dir, trainSrc, train_subdirs)\n",
    "# copy_testFiles_to_raw(images_ts_dir, testSrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesting. label integrity check fails. Expected: [np.int64(0), np.int64(1), np.int64(2)] Found: [0.        1.0000153 2.       ]\n",
    "# fixing the label files\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "\n",
    "# Function to correct labels\n",
    "def correct_labels(image):\n",
    "    array = sitk.GetArrayFromImage(image)\n",
    "    array = np.where(np.isclose(array, 1.0000153), 1, array)  # Correcting the label value\n",
    "    array = np.int64(array)\n",
    "    corrected_image = sitk.GetImageFromArray(array)\n",
    "    corrected_image.CopyInformation(image)\n",
    "    return corrected_image\n",
    "\n",
    "# Load the problematic image\n",
    "def correct_all_type_labels(labels_tr_dir):\n",
    "    labelImagesInDir = [sdir for sdir in sorted(os.listdir(labels_tr_dir)) if not sdir.startswith('.')]\n",
    "\n",
    "    for labelImgIdx, labelImg in enumerate(labelImagesInDir):\n",
    "        imagePath = os.path.join(labels_tr_dir, labelImg)\n",
    "        image = sitk.ReadImage(imagePath)\n",
    "\n",
    "    # Correct the labels\n",
    "        corrected_image = correct_labels(image)\n",
    "        sitk.WriteImage(corrected_image, imagePath)\n",
    "\n",
    "\n",
    "# ## uncomment to run ###\n",
    "# correct_all_type_labels(labels_tr_dir)\n",
    "\n",
    "# create dataset.json\n",
    "channel_names = {0: \"CT\"}\n",
    "\n",
    "labels = {\n",
    "    'background': 0,\n",
    "    'pancreas': 1,\n",
    "    'lesion': 2\n",
    "}\n",
    "\n",
    "num_training_cases = len(os.listdir(images_tr_dir))\n",
    "file_ending = '.nii.gz'\n",
    "\n",
    "# ## uncomment to run ###\n",
    "# generate_dataset_json.generate_dataset_json(nnunet_base, channel_names, labels, num_training_cases, file_ending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spacing(image_path, seg_path):\n",
    "    # Load the image and segmentation\n",
    "    image = sitk.ReadImage(image_path)\n",
    "    seg = sitk.ReadImage(seg_path)\n",
    "    \n",
    "    # Get the spacing from the image\n",
    "    image_spacing = image.GetSpacing()\n",
    "    seg_spacing = seg.GetSpacing()\n",
    "    \n",
    "    # Compare and correct spacing if needed\n",
    "    if not np.allclose(image_spacing, seg_spacing, atol=1e-7):\n",
    "        print(f\"Correcting spacing for {seg_path}\")\n",
    "        seg.SetSpacing(image_spacing)\n",
    "        \n",
    "        # Save the corrected segmentation\n",
    "        sitk.WriteImage(seg, seg_path)\n",
    "        print(f\"Corrected segmentation saved to {seg_path}\")\n",
    "\n",
    "# Correct spacing for the segmentation files\n",
    "# list all files under imagesTr\n",
    "\n",
    "#  ## uncomment to run ###\n",
    "# imageFilesInDir = [sdir for sdir in sorted(os.listdir(images_tr_dir)) if not sdir.startswith('.')]\n",
    "\n",
    "# for imageIdx, image in enumerate(imageFilesInDir):\n",
    "#     imgPath = os.path.join(images_tr_dir, image)\n",
    "#     segPath = os.path.join(labels_tr_dir, image.replace('_0000', ''))\n",
    "\n",
    "#     correct_spacing(imgPath, segPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "\n",
    "# Define the necessary arguments\n",
    "# args = [\n",
    "#     \"script_name\",  # This is a placeholder for the script name\n",
    "#     \"Dataset876_UHNMedImg3D\",  # dataset_name_or_id\n",
    "#     \"3d_fullres\",  # configuration\n",
    "#     f\"{fold}\",  # fold\n",
    "#     # '-tr', 'nnUNetTrainer',  # optional: trainer_class_name\n",
    "#     # '-p', 'nnUNetPlans',  # optional: plans_identifier\n",
    "# ]\n",
    "\n",
    "args = [\n",
    "    \"script_name\",  # This is a placeholder for the script name\n",
    "    \"Dataset876_UHNMedImg3D\",  # dataset_name_or_id\n",
    "    \"3d_fullres\",  # configuration\n",
    "    f\"{fold}\",  # fold\n",
    "    '-tr', 'nnUnetSegClsTrainer',  # optional: trainer_class_name\n",
    "    # '-p', 'nnUNetPlans',  # optional: plans_identifier\n",
    "]\n",
    "\n",
    "# Set sys.argv to the list of arguments\n",
    "sys.argv = args\n",
    "\n",
    "# Run the training entry function\n",
    "run_training_entry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnUNet_raw is not defined and nnU-Net can only be used on data for which preprocessed files are already present on your system. nnU-Net cannot be used for experiment planning and preprocessing like this. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up properly.\n",
      "nnUNet_preprocessed is not defined and nnU-Net can not be used for preprocessing or training. If this is not intended, please read documentation/setting_up_paths.md for information on how to set this up.\n",
      "nnUNet_results is not defined and nnU-Net cannot be used for training or inference. If this is not intended behavior, please read documentation/setting_up_paths.md for information on how to set this up.\n",
      "Fingerprint extraction...\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/uhn_venv/bin/nnUNetv2_plan_and_preprocess\", line 8, in <module>\n",
      "    sys.exit(plan_and_preprocess_entry())\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bhatti_uhn/nnU-net/nnunetv2/experiment_planning/plan_and_preprocess_entrypoints.py\", line 180, in plan_and_preprocess_entry\n",
      "    extract_fingerprints(args.d, args.fpe, args.npfp, args.verify_dataset_integrity, args.clean, args.verbose)\n",
      "  File \"/home/bhatti_uhn/nnU-net/nnunetv2/experiment_planning/plan_and_preprocess_api.py\", line 47, in extract_fingerprints\n",
      "    extract_fingerprint_dataset(d, fingerprint_extractor_class, num_processes, check_dataset_integrity, clean,\n",
      "  File \"/home/bhatti_uhn/nnU-net/nnunetv2/experiment_planning/plan_and_preprocess_api.py\", line 26, in extract_fingerprint_dataset\n",
      "    dataset_name = convert_id_to_dataset_name(dataset_id)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/bhatti_uhn/nnU-net/nnunetv2/utilities/dataset_name_id_conversion.py\", line 48, in convert_id_to_dataset_name\n",
      "    raise RuntimeError(f\"Could not find a dataset with the ID {dataset_id}. Make sure the requested dataset ID \"\n",
      "RuntimeError: Could not find a dataset with the ID 876. Make sure the requested dataset ID exists and that nnU-Net knows where raw and preprocessed data are located (see Documentation - Installation). Here are your currently defined folders:\n",
      "nnUNet_preprocessed=None\n",
      "nnUNet_results=None\n",
      "nnUNet_raw=None\n",
      "If something is not right, adapt your environment variables.\n"
     ]
    }
   ],
   "source": [
    "!nnUNetv2_plan_and_preprocess -d 876 --verify_dataset_integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the \"/home/bhatti_uhn/nnUNet_results/Dataset876_UHNMedImg3D/nnUnetSegClsTrainer__nnUNetPlans__3d_fullres/fold_all/validation\" directory and classification.npy file.\n",
    "# store in a df and save to csv\n",
    "# df template: Name | Label | SubType (Prediction)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to the classification file\n",
    "valSrcPath = \"/home/bhatti_uhn/nnUNet_results/Dataset876_UHNMedImg3D/nnUnetSegClsTrainer__nnUNetPlans__3d_fullres/fold_all/validation\"\n",
    "\n",
    "# list classification.npy files\n",
    "classification_files = [sdir for sdir in sorted(os.listdir(valSrcPath)) if sdir.split('_')[-1] == 'classification.npy']\n",
    "\n",
    "valResults = {}\n",
    "df = pd.DataFrame(columns=['Name', 'Label', 'SubType'])\n",
    "\n",
    "for classFile in classification_files:\n",
    "    classFilePath = os.path.join(valSrcPath, classFile)\n",
    "    # print(f\"Processing {classFilePath}\")\n",
    "\n",
    "    # target class\n",
    "    target_class = int(classFile.split('_')[1])\n",
    "\n",
    "    # Load the classification file\n",
    "    classification = np.load(classFilePath)\n",
    "    # keep argmax only\n",
    "    class_pred = np.argmax(classification, axis=-1)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    new_row = pd.DataFrame({'Name': [classFile.replace('_classification.npy', '.nii.gz')], 'Label': [int(target_class)], 'SubType': class_pred})\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    # # Save the DataFrame to a CSV file\n",
    "    # df.to_csv(classFilePath.replace('.npy', '.csv'), index=False)\n",
    "\n",
    "    # print(f\"Saved classification to {classFilePath.replace('.npy', '.csv')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.996031746031746\n"
     ]
    }
   ],
   "source": [
    "# compare label and subType and calculate accuracy\n",
    "df['Correct'] = df['Label'] == df['SubType']\n",
    "accuracy = df['Correct'].sum() / len(df)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Label</th>\n",
       "      <th>SubType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>quiz_0_041.nii.gz</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quiz_0_060.nii.gz</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>quiz_0_066.nii.gz</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>quiz_0_070.nii.gz</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>quiz_0_077.nii.gz</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>quiz_2_497.nii.gz</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>quiz_2_501.nii.gz</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>quiz_2_508.nii.gz</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>quiz_2_527.nii.gz</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>quiz_2_535.nii.gz</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name Label SubType\n",
       "0    quiz_0_041.nii.gz     0       0\n",
       "1    quiz_0_060.nii.gz     0       0\n",
       "2    quiz_0_066.nii.gz     0       0\n",
       "3    quiz_0_070.nii.gz     0       0\n",
       "4    quiz_0_077.nii.gz     0       0\n",
       "..                 ...   ...     ...\n",
       "247  quiz_2_497.nii.gz     2       2\n",
       "248  quiz_2_501.nii.gz     2       2\n",
       "249  quiz_2_508.nii.gz     2       2\n",
       "250  quiz_2_527.nii.gz     2       2\n",
       "251  quiz_2_535.nii.gz     2       2\n",
       "\n",
       "[252 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 320, 320], 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'strides': [[1, 1, 1], [1, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}\n",
      "There are 72 cases in the source folder\n",
      "I am process 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 72 cases that I would like to predict\n",
      "overwrite was set to False, so I am only working on cases that haven't been predicted yet. That's 72 cases.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'generator' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 31\u001b[0m\n\u001b[1;32m     22\u001b[0m predictor\u001b[38;5;241m.\u001b[39minitialize_from_trained_model_folder(\n\u001b[1;32m     23\u001b[0m     join(\n\u001b[1;32m     24\u001b[0m         nnUNet_results,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     checkpoint_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint_final.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# variant 1: give input and output folders\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_from_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnnUNet_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDataset876_UHNMedImg3D/imagesTs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnnUNet_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDataset876_UHNMedImg3D/imagesTs_3d_fullres\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_probabilities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_processes_preprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_processes_segmentation_export\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfolder_with_segs_from_prev_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_parts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpart_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/bhatti_uhn/nnU-net/nnunetv2/inference/predict_from_raw_data_cls.py:128\u001b[0m, in \u001b[0;36mnnUNetSegClsPredictor.predict_from_files\u001b[0;34m(self, list_of_lists_or_source_folder, output_folder_or_list_of_truncated_output_files, save_probabilities, overwrite, num_processes_preprocessing, num_processes_segmentation_export, folder_with_segs_from_prev_stage, num_parts, part_id)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    123\u001b[0m data_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_get_data_iterator_from_lists_of_filenames(list_of_lists_or_source_folder,\n\u001b[1;32m    124\u001b[0m                                                                          seg_from_prev_stage_files,\n\u001b[1;32m    125\u001b[0m                                                                          output_filename_truncated,\n\u001b[1;32m    126\u001b[0m                                                                          num_processes_preprocessing)\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_from_data_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_probabilities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_processes_segmentation_export\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/bhatti_uhn/nnU-net/nnunetv2/inference/predict_from_raw_data_cls.py:141\u001b[0m, in \u001b[0;36mnnUNetSegClsPredictor.predict_from_data_iterator\u001b[0;34m(self, data_iterator, save_probabilities, num_processes_segmentation_export)\u001b[0m\n\u001b[1;32m    139\u001b[0m worker_list \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m export_pool\u001b[38;5;241m.\u001b[39m_pool]\n\u001b[1;32m    140\u001b[0m r \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 141\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m##### list of keys in the data iterator: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdata_iterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m preprocessed \u001b[38;5;129;01min\u001b[39;00m data_iterator:\n\u001b[1;32m    144\u001b[0m     data \u001b[38;5;241m=\u001b[39m preprocessed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'generator' object has no attribute 'keys'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from nnunetv2.paths import nnUNet_results, nnUNet_raw\n",
    "import torch\n",
    "from batchgenerators.utilities.file_and_folder_operations import join\n",
    "from nnunetv2.inference.predict_from_raw_data_cls import nnUNetSegClsPredictor\n",
    "from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO\n",
    "\n",
    "# nnUNetv2_predict -d 3 -f 0 -c 3d_lowres -i imagesTs -o imagesTs_predlowres --continue_prediction\n",
    "# /home/bhatti_uhn/nnUNet_results/Dataset876_UHNMedImg3D/nnUnetSegClsTrainer__nnUNetPlans__3d_fullres\n",
    "# /home/bhatti_uhn/nnUNet_raw/Dataset876_UHNMedImg3D/imagesTs\n",
    "# instantiate the nnUNetPredictor\n",
    "predictor = nnUNetSegClsPredictor(\n",
    "    tile_step_size=0.5,\n",
    "    use_gaussian=True,\n",
    "    use_mirroring=True,\n",
    "    perform_everything_on_device=True,\n",
    "    device=torch.device(\"cuda\", 0),\n",
    "    verbose=False,\n",
    "    verbose_preprocessing=False,\n",
    "    allow_tqdm=True,\n",
    ")\n",
    "# initializes the network architecture, loads the checkpoint\n",
    "predictor.initialize_from_trained_model_folder(\n",
    "    join(\n",
    "        nnUNet_results,\n",
    "        \"Dataset876_UHNMedImg3D/nnUnetSegClsTrainer__nnUNetPlans__3d_fullres\",\n",
    "    ),\n",
    "    use_folds=\"all\",\n",
    "    checkpoint_name=\"checkpoint_final.pth\",\n",
    ")\n",
    "# variant 1: give input and output folders\n",
    "predictor.predict_from_files(\n",
    "    join(nnUNet_raw, \"Dataset876_UHNMedImg3D/imagesTs\"),\n",
    "    join(nnUNet_raw, \"Dataset876_UHNMedImg3D/imagesTs_3d_fullres\"),\n",
    "    save_probabilities=False,\n",
    "    overwrite=False,\n",
    "    num_processes_preprocessing=2,\n",
    "    num_processes_segmentation_export=2,\n",
    "    folder_with_segs_from_prev_stage=None,\n",
    "    num_parts=1,\n",
    "    part_id=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    from nnunetv2.paths import nnUNet_results, nnUNet_raw\n",
    "    import torch\n",
    "    from batchgenerators.utilities.file_and_folder_operations import join\n",
    "    from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n",
    "    from nnunetv2.imageio.simpleitk_reader_writer import SimpleITKIO\n",
    "\n",
    "    # nnUNetv2_predict -d 3 -f 0 -c 3d_lowres -i imagesTs -o imagesTs_predlowres --continue_prediction\n",
    "\n",
    "    # instantiate the nnUNetPredictor\n",
    "    predictor = nnUNetPredictor(\n",
    "        tile_step_size=0.5,\n",
    "        use_gaussian=True,\n",
    "        use_mirroring=True,\n",
    "        perform_everything_on_device=True,\n",
    "        device=torch.device('cuda', 0),\n",
    "        verbose=False,\n",
    "        verbose_preprocessing=False,\n",
    "        allow_tqdm=True\n",
    "    )\n",
    "    # initializes the network architecture, loads the checkpoint\n",
    "    predictor.initialize_from_trained_model_folder(\n",
    "        join(nnUNet_results, 'Dataset003_Liver/nnUNetTrainer__nnUNetPlans__3d_lowres'),\n",
    "        use_folds=(0,),\n",
    "        checkpoint_name='checkpoint_final.pth',\n",
    "    )\n",
    "    # variant 1: give input and output folders\n",
    "    predictor.predict_from_files(join(nnUNet_raw, 'Dataset003_Liver/imagesTs'),\n",
    "                                 join(nnUNet_raw, 'Dataset003_Liver/imagesTs_predlowres'),\n",
    "                                 save_probabilities=False, overwrite=False,\n",
    "                                 num_processes_preprocessing=2, num_processes_segmentation_export=2,\n",
    "                                 folder_with_segs_from_prev_stage=None, num_parts=1, part_id=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uhn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
